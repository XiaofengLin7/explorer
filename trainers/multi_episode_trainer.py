"""Custom trainer that extracts multi-episode metrics.

This trainer extends AgentPPOTrainer to extract additional metrics from
environments that have a get_metrics() method, such as MultiEpisodeEnv.
"""

from __future__ import annotations

import json
import os
import uuid
from typing import Any

import numpy as np
import torch
from omegaconf import OmegaConf
from verl import DataProto  # type: ignore

from rllm.engine.agent_execution_engine import AsyncAgentExecutionEngine
from rllm.trainer.verl.agent_ppo_trainer import AgentPPOTrainer


class MultiEpisodeAsyncAgentExecutionEngine(AsyncAgentExecutionEngine):
    """Execution engine that extracts metrics from environments with get_metrics()."""

    async def run_agent_trajectory_async(self, idx, application_id, seed=0, mode="Text", **kwargs):
        """Run trajectory and include environment metrics if available."""
        # Store reference to env before it might be modified
        env = self.envs[idx]

        # Call parent method
        result = await super().run_agent_trajectory_async(
            idx, application_id, seed=seed, mode=mode, **kwargs
        )

        # If Token mode, extract additional metrics from environment
        if mode == "Token" and isinstance(result, dict) and "metrics" in result:
            if hasattr(env, "get_metrics") and callable(env.get_metrics):
                try:
                    env_metrics = env.get_metrics()
                    if isinstance(env_metrics, dict):
                        # Flatten nested keys and add to metrics
                        for key, value in env_metrics.items():
                            flat_key = key.replace("/", "_")
                            result["metrics"][flat_key] = value
                except Exception:
                    pass  # Silently ignore if metrics extraction fails

        return result


class MultiEpisodeAgentPPOTrainer(AgentPPOTrainer):
    """Trainer that uses MultiEpisodeAsyncAgentExecutionEngine for metrics extraction."""

    def init_workers(self):
        """Initialize workers with custom execution engine."""
        # Call grandparent's init_workers (skip AgentPPOTrainer's)
        from verl.trainer.ppo.ray_trainer import RayPPOTrainer

        RayPPOTrainer.init_workers(self)

        engine_args = OmegaConf.to_container(self.config.rllm.agent.get("engine_args", {})) or {}
        n_parallel_agents = (
            engine_args.pop("n_parallel_agents", None)
            or self.config.data.train_batch_size * self.config.actor_rollout_ref.rollout.n
        )
        print(f"n_parallel_agents: {n_parallel_agents}")

        # Use our custom execution engine instead
        self.agent_execution_engine = MultiEpisodeAsyncAgentExecutionEngine(
            rollout_engine=self.async_rollout_manager,
            config=self.config,
            engine_name="verl",
            tokenizer=self.tokenizer,
            model_path=self.config.actor_rollout_ref.model.path,
            max_steps=self.config.rllm.agent.max_steps,
            max_response_length=self.config.data.max_response_length,
            max_prompt_length=self.config.data.max_prompt_length,
            agent_class=self.agent_class,
            agent_args=self.agent_args,
            env_class=self.env_class,
            env_args=self.env_args,
            enforce_max_prompt_length=self.config.rllm.stepwise_advantage.enable,
            trajectory_timeout=self.config.rllm.agent.trajectory_timeout,
            overlong_filter=self.config.rllm.agent.get("overlong_filter", False),
            disable_thinking=self.config.rllm.disable_thinking,
            n_parallel_agents=n_parallel_agents,
            **engine_args,
        )

    def _validate_agent(self):
        """Override validation to include environment metrics from MultiEpisodeEnv."""
        rewards_lst = []
        data_source_lst = []
        uid_lst = []
        env_metrics_lst = []  # Collect environment metrics
        for test_data in self.val_dataloader:
            test_batch = DataProto.from_single_dict(test_data)
            test_batch.non_tensor_batch["uid"] = np.array([str(uuid.uuid4()) for _ in range(len(test_batch.batch))], dtype=object)
            n_val_samples = self.config.actor_rollout_ref.rollout.val_kwargs.n
            test_batch = test_batch.repeat(repeat_times=n_val_samples, interleave=True)
            test_batch.pop(["input_ids", "attention_mask", "position_ids"])  # these are not needed for environment based interaction
            test_batch.meta_info = {
                "eos_token_id": self.tokenizer.eos_token_id,
                "pad_token_id": self.tokenizer.pad_token_id,
                "recompute_log_prob": False,
                "do_sample": False,
                "validate": True,
            }
            self.init_envs_and_agents(test_batch)

            if self.config.rllm.stepwise_advantage.enable:
                test_output_gen_batch = self.generate_agent_steps(meta_info=test_batch.meta_info, uids=test_batch.non_tensor_batch["uid"])
                # for validation, we only need the last step
                is_last_step = test_output_gen_batch.non_tensor_batch["is_last_step"]
                last_step_indices = np.where(is_last_step == True)[0]
                test_output_gen_batch = test_output_gen_batch.select_idxs(last_step_indices)  # This batch only has last steps
            else:
                test_output_gen_batch, _ = self.generate_agent_trajectory(meta_info=test_batch.meta_info)

            test_batch = test_batch.union(test_output_gen_batch)

            reward_tensor = test_batch.batch["token_level_scores"]

            rewards_lst.append(reward_tensor.sum(-1).cpu())
            data_source_lst.append(test_batch.non_tensor_batch.get("data_source", ["unknown"] * reward_tensor.shape[0]))
            uid_lst.append(test_batch.non_tensor_batch["uid"])

            # Collect environment metrics if available
            if hasattr(self.agent_execution_engine, "envs") and self.agent_execution_engine.envs:
                batch_env_metrics = []
                for env in self.agent_execution_engine.envs:
                    if hasattr(env, "get_metrics") and callable(env.get_metrics):
                        try:
                            env_metrics = env.get_metrics()
                            if isinstance(env_metrics, dict):
                                batch_env_metrics.append(env_metrics)
                        except Exception:
                            pass  # Silently ignore if metrics extraction fails
                env_metrics_lst.append(batch_env_metrics)

        reward_tensor = torch.cat(rewards_lst, dim=0)  # (batch_size,)
        data_sources = np.concatenate(data_source_lst, axis=0)
        # evaluate test_score based on data source
        data_source_reward = {}

        # to group for pass@k
        uid_tensor = np.concatenate(uid_lst, axis=0)
        data_source_uid_pass_rates = {}  # data source to {uid: pass or not}

        for i in range(reward_tensor.shape[0]):
            data_source = data_sources[i]

            if data_source not in data_source_reward:
                data_source_reward[data_source] = []
            data_source_reward[data_source].append(reward_tensor[i].item())

            # pass@k
            if data_source not in data_source_uid_pass_rates:
                data_source_uid_pass_rates[data_source] = {}

            uid = uid_tensor[i]
            if uid not in data_source_uid_pass_rates[data_source]:
                data_source_uid_pass_rates[data_source][uid] = 0  # default to not pass
            # take highest score
            data_source_uid_pass_rates[data_source][uid] = max(data_source_uid_pass_rates[data_source][uid], reward_tensor[i].item())

        metric_dict = {}
        for data_source, rewards in data_source_reward.items():
            # clip rewards to be between 0 and 1
            rewards_array = np.array(rewards)
            rewards_array = np.clip(rewards_array, 0, 1)
            metric_dict[f"val/test_score/{data_source}"] = np.mean(rewards_array)

        for data_source, pass_rates in data_source_uid_pass_rates.items():
            pass_k_lst = []
            for uid, pass_score in pass_rates.items():
                pass_k_lst.append(pass_score >= 1)  # assuming 1 means passed
            metric_dict[f"val/test_score/pass@k/{data_source}"] = np.mean(pass_k_lst)

        # Aggregate environment metrics if available
        if env_metrics_lst:
            # Flatten list of lists
            all_env_metrics = [metrics for batch_metrics in env_metrics_lst for metrics in batch_metrics]
            if all_env_metrics:
                # Aggregate metrics across all environments
                # Collect all metric keys
                all_keys = set()
                for metrics in all_env_metrics:
                    all_keys.update(metrics.keys())

                # Aggregate each metric (filter out -1 values as they indicate missing episodes)
                for key in all_keys:
                    values = []
                    for metrics in all_env_metrics:
                        if key in metrics:
                            value = metrics[key]
                            # Filter out -1 values (missing episodes)
                            if isinstance(value, (int, float)) and value >= 0:
                                values.append(float(value))
                    if values:
                        metric_dict[f"val/{key}"] = np.mean(values)

        return metric_dict

